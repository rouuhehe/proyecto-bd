# PROYECTO BASE DE DATOS ğŸ’¯

## Cargar base de datos

Holi, grupo, en el repo encontrarÃ¡n el archivo `backup_bd.dump`, para cargar la base de datos hay dos opciones, desde terminal o usando pgAdmin, en esta secciÃ³n solo mostrarÃ© como se hace desde pgAdmin porque ya tengo sueÃ±o ğŸ˜¢.

### Usando pgAdmin 

1. Hagan click derecho sobre `Databases`, luego a `Create` y, finalmente, en `Database` le ponen un nombre, en mi caso serÃ¡ **prueba**

![alt text](readme-files/image.png)
![alt text](readme-files/image-1.png)

2.  Ahora le dan click derecho a la nueva db y luego a `Restore`

![alt text](readme-files/image-2.png)

3. El formato dejenlo en custom y en `filename` busquen el archivo en la carpeta del repo.

![alt text](readme-files/image-4.png)

En caso no les aparezca, cambien esa cosita a `All files`
![alt text](readme-files/image-3.png)

Â¡Eso es todo! ğŸ”¥

## Crear base de datos

En caso quieran poblar la base de datos por su cuenta, en el archivo `crear_tablas.sql` encontrarÃ¡n el script para crear las tablas de la db. Si usan Visual Studio Code pueden instalar la extensiÃ³n de `SQLTools`, `SQLTools PostgreSQL/otras-cosas` y ejecutar el script directamente desde ahÃ­. En caso de que no tengan la extensiÃ³n, pueden copiar el script y pegarlo en el editor de consultas de pgAdmin.

## Generar e insertar datos

Para generar datos de prueba, usaremos la librerÃ­a `Faker` y `psycopg2` para conectarnos a la base de datos PostgreSQL. AsegÃºrense de tener instalado el paquete `Faker` y `psycopg2-binary`. Pueden instalarlo ejecutando el siguiente comando en su terminal:

```
pip install faker psycopg2-binary
```

Luego en el archivo de python, asegÃºrense de establecer la conexiÃ³n a la base de datos. AquÃ­ les dejo un ejemplo de cÃ³mo hacerlo (de todas formas en todos los archivos se encuentra esta cabecera, aun asÃ­, revisen el puerto):

```python
conn = psycopg2.connect(
    dbname="proyecto", # Nombre de la base de datos
    user="postgres", # Usuario de la base de datos
    password="postgres", # ContraseÃ±a del usuario
    host="localhost", 
    port="5433" # Puerto de la base de datos, lo configuraron en el pgAdmin
)
```

Una vez establecida la conexiÃ³n, pueden ejecutar el script `cargar_nDatos.py` para generar e insertar datos en las tablas. Este script generarÃ¡ datos de prueba para las tablas que utilizaremos en las consultas.

Pueden correr los scripts como mejor se les acomode.

## Eliminar datos
Si necesitan eliminar los datos de las tablas, pueden usar el script `eliminar_datos.sql` que se encuentra en el repositorio. Este script eliminarÃ¡ todos los datos de las tablas sin eliminar las tablas mismas.

# Acerca del enunciado

## Consideraciones

Se nos dice que debemos considerar que la **â€œexperimentacion**" y "**optimizacion**â€ debe realizarse mediante 2 o 3 consultas
â€œ**genericas**â€ con un **nivel aceptable de complejidad en las consultas propuestas**. 

Dichas consultas se deben realizar en cuatro contextos de 1000 (mil) datos, 10000 (diez mil) datos, 100000 (cien mil) datos y 1 000 000 (un millon) de datos almacenados en la base de datos (deberÄ±an tener 4 dumps de su proyecto).

---

Usaremos estas tres consultas como base para la comparaciÃ³n:
1. Â¿QuÃ© veterinarios han atendido la mayor cantidad de mascotas en el Ãºltimo mes?
2. Â¿CuÃ¡l es el tipo de tratamiento mÃ¡s comÃºn a perros en los Ãºltimos seis meses?
3. Â¿QuÃ© dÃ­a de la semana concentra el mayor nÃºmero de citas mÃ©dicas?


(Extra) Â¿Cual serÄ±a la complejidad operacional si escalamos los datos por encima del millon?,
realice una comparativa respecto a la cantidad de datos del p Ìarrafo anterior. Â¿Es suficiente la arquitectura Cliente-Servidor para procesar millones de datos?

